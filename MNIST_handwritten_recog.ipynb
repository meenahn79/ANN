{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST handwritten recog.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJHKUa/pCNtaQQfpQ0CzHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meenahn79/ANN/blob/master/MNIST_handwritten_recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVwGISHKdIE",
        "colab_type": "code",
        "outputId": "421c26cf-7787-47b7-fc45-1bd0573ad5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf # Import tensorflow library\n",
        "import matplotlib.pyplot as plt # Import matplotlib library\n",
        "import numpy as np # Import numpy library\n",
        "from keras.utils import np_utils\n",
        "import keras.models\n",
        "from keras.layers import Activation, Dense\n",
        "import seaborn as sns\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        " # Object of the MNIST dataset \n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data() # Load data\n",
        "\n",
        "plt.imshow(x_train[9], cmap=\"gray\") # Import the image\n",
        "plt.show() # Plot the image\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]) # Normalize the training dataset\n",
        "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]) # Normalize the testing dataset\n",
        "\n",
        "x_train=x_train/255\n",
        "x_test= x_test/255\n",
        "\n",
        "print(\"number of training example =\",x_train.shape[0],\"shape of each image=\",x_train.shape[1])\n",
        "print(\"number of testing example =\",x_test.shape[0],\"shape of each image=\",x_test.shape[1])\n",
        "print(x_train[0])\n",
        "\n",
        "print(\"class label of the first image\",y_train[0])\n",
        "y_train=np_utils.to_categorical(y_train,10)\n",
        "y_test=np_utils.to_categorical(y_test,10)\n",
        "print(\"After converting the output to vector\", y_train[0])\n",
        "\n",
        "#model=keras.models.Sequential([Dense(32,input_shape=(784,)),Activation('relu'),Dense(10),Activation('softmax')])\n",
        "model=keras.models.Sequential()\n",
        "model.add(Dense(10, input_shape=(784,)))\n",
        "\n",
        "# Afterwards, we do automatic shape inference:\n",
        "model.add(Dense(10))\n",
        " # Start training process\n",
        "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x=x_train, y=y_train, batch_size=512, epochs=10, verbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model performance\n",
        "test_loss, test_acc = model.evaluate(x=x_test, y=y_test)\n",
        "\n",
        "# Print out the model accuracy \n",
        "print('\\nTest accuracy:', test_acc)\n",
        "predictions = model.predict_classes([x_test]) # Make prediction\n",
        "print\n",
        "#print(np.argmax(predictions[19])) # Print out the number\n",
        "#plt.imshow(x_test[1], cmap=\"gray\") # Import the image\n",
        "plt.show()\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "number of training example = 60000 shape of each image= 784\n",
            "number of testing example = 10000 shape of each image= 784\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "class label of the first image 5\n",
            "After converting the output to vector [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/10\n",
            "48000/48000 [==============================] - 1s 12us/step - loss: 3.3176 - accuracy: 0.1643 - val_loss: 3.0227 - val_accuracy: 0.1397\n",
            "Epoch 2/10\n",
            "48000/48000 [==============================] - 1s 10us/step - loss: 6.2118 - accuracy: 0.2096 - val_loss: 6.9739 - val_accuracy: 0.2983\n",
            "Epoch 3/10\n",
            "48000/48000 [==============================] - 1s 11us/step - loss: 6.6419 - accuracy: 0.2133 - val_loss: 4.5550 - val_accuracy: 0.0974\n",
            "Epoch 4/10\n",
            "48000/48000 [==============================] - 0s 10us/step - loss: 6.5840 - accuracy: 0.1358 - val_loss: 5.9208 - val_accuracy: 0.1528\n",
            "Epoch 5/10\n",
            "48000/48000 [==============================] - 1s 11us/step - loss: 7.3613 - accuracy: 0.1614 - val_loss: 6.6243 - val_accuracy: 0.1687\n",
            "Epoch 6/10\n",
            "48000/48000 [==============================] - 1s 11us/step - loss: 7.1243 - accuracy: 0.1741 - val_loss: 8.4115 - val_accuracy: 0.1971\n",
            "Epoch 7/10\n",
            "48000/48000 [==============================] - 1s 10us/step - loss: 8.0244 - accuracy: 0.1966 - val_loss: 7.9398 - val_accuracy: 0.1945\n",
            "Epoch 8/10\n",
            "48000/48000 [==============================] - 0s 10us/step - loss: 7.7251 - accuracy: 0.1919 - val_loss: 8.2428 - val_accuracy: 0.1887\n",
            "Epoch 9/10\n",
            "48000/48000 [==============================] - 0s 10us/step - loss: 8.1108 - accuracy: 0.1880 - val_loss: 7.9380 - val_accuracy: 0.1937\n",
            "Epoch 10/10\n",
            "48000/48000 [==============================] - 0s 10us/step - loss: 7.7868 - accuracy: 0.1929 - val_loss: 6.9652 - val_accuracy: 0.1935\n",
            "10000/10000 [==============================] - 0s 21us/step\n",
            "\n",
            "Test accuracy: 0.19460000097751617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85srCK3Gh9r4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}