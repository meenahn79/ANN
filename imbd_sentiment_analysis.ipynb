{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imbd sentiment analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8TsEnylylbOBQ41U5zrOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meenahn79/ANN/blob/master/imbd_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6__R7QocKeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "a1be8873-06b8-4459-bcd5-78531aeae10b"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Flatten, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_words = 2000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=num_words,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "max_review_length = 250\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "train_history = model.fit(X_train, y_train, batch_size=32,epochs=1, verbose=2,  validation_split=0.2)\n",
        "\n",
        "def show_train_history(train_history,train,validation):\n",
        "    plt.plot(train_history.history[train])\n",
        "    plt.plot(train_history.history[validation])\n",
        "    plt.title('Train History')\n",
        "    plt.ylabel(train)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "#show_train_history(train_history,'acc','val_acc')\n",
        "#show_train_history(train_history,'loss','val_loss')\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "scores[1]\n",
        "\n",
        "predict=model.predict_classes(X_test)\n",
        "predict_classes=predict.reshape(len(X_test))\n",
        "\n",
        "def get_original_text(i):\n",
        "    word_to_id = imdb.get_word_index()\n",
        "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "    return ' '.join(id_to_word[id] for id in X_test[i])\n",
        "\n",
        " \n",
        "def display_test_sentiment(i):\n",
        "    SentimentDict= {1:'positive', 0:'negative'}\n",
        "    print(get_original_text(i))\n",
        "    print('label: ', SentimentDict[y_test[i]], ', prediction: ', SentimentDict[predict_classes[i]])\n",
        "\n",
        "\n",
        "display_test_sentiment(3)\n",
        "display_test_sentiment(13000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 250, 32)           64000     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               8448      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 81,025\n",
            "Trainable params: 81,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            " - 83s - loss: 0.4425 - accuracy: 0.7889 - val_loss: 0.3284 - val_accuracy: 0.8630\n",
            "25000/25000 [==============================] - 17s 693us/step\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> i generally love this type of movie however this time i found myself wanting to kick the screen since i can't do that i will just <UNK> about it this was absolutely <UNK> the things that happen with the dead kids are very cool but the alive people are absolute <UNK> i am a <UNK> man pretty big and i can <UNK> myself well however i would not do half the stuff the little girl does in this movie also the mother in this movie is <UNK> with her children to the point of <UNK> i wish i wasn't so angry about her and her actions because i would have otherwise enjoyed the flick what a number she was take my <UNK> and fast forward through everything you see her do until the end also is anyone else getting sick of watching movies that are filmed so dark anymore one can hardly see what is being filmed as an audience we are <UNK> involved with the actions on the screen so then why the hell can't we have night vision\n",
            "label:  negative , prediction:  positive\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film brought me to tears i have to say that if i did not have a beautiful husband at home i would ask this beautiful piece of art to <UNK> me <UNK> <UNK> gives a <UNK> performance as a confused young pop star while <UNK> <UNK> <UNK> quick and witty dialogue that only <UNK> the genius of <UNK> performance <UNK> is pretty gay but his performance was nothing less than <UNK> he is also very <UNK> and cute i'm thinking about <UNK> him out on a date and giving him a very sweet <UNK> <UNK> br br if you would like to <UNK> this film in the future please <UNK> me br br nick <UNK> class of <UNK> <UNK> <UNK> <UNK> west\n",
            "label:  positive , prediction:  positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}